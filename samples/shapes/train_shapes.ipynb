{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaqu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\joaqu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\joaqu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\joaqu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\joaqu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\joaqu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pretrained model to C:\\Users\\joaqu\\mask_RCNN\\mask_rcnn_coco.h5 ...\n",
      "... done downloading pretrained model!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joaqu\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaqu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\joaqu\\mask_RCNN\\mrcnn\\model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaqu\\mask_RCNN\\mrcnn\\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaqu\\mask_RCNN\\mrcnn\\model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You need the TensorFlow (v1) module installed to use TensorBoard.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\callbacks\\tensorboard_v1.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster_resolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstrained_optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\compiler\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxla\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\compiler\\xla.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxla\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxla\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_fn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\model_fn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdnn_logit_fn_builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkmeans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeansClustering\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_features_v2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dense_features' from 'tensorflow.python.feature_column' (C:\\Users\\joaqu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             layers='heads')\n\u001b[0m",
      "\u001b[1;32m~\\mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         callbacks = [\n\u001b[0;32m   2340\u001b[0m             keras.callbacks.TensorBoard(log_dir=self.log_dir,\n\u001b[1;32m-> 2341\u001b[1;33m                                         histogram_freq=0, write_graph=True, write_images=False),\n\u001b[0m\u001b[0;32m   2342\u001b[0m             keras.callbacks.ModelCheckpoint(self.checkpoint_path,\n\u001b[0;32m   2343\u001b[0m                                             verbose=0, save_weights_only=True),\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\callbacks\\tensorboard_v1.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             raise ImportError('You need the TensorFlow (v1) module installed to '\n\u001b[0m\u001b[0;32m     99\u001b[0m                               'use TensorBoard.')\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: You need the TensorFlow (v1) module installed to use TensorBoard."
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
